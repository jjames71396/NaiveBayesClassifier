{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment3.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LcsOPmxBO94k"
      },
      "source": [
        "<h1 style=\"text-align: center;\">Assignment 3</h1>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WPiyDeaOT_YD"
      },
      "source": [
        "import numpy as np"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RrbiaOc2UDfS"
      },
      "source": [
        "data = []\n",
        "with open('imdb_labelled.txt') as f:\n",
        "    data = f.readlines()"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3MytkvEoXxlv"
      },
      "source": [
        "<p>Split the dataset into train/dev/test </p>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EtLkByLgXxK0"
      },
      "source": [
        "train_data = data[:720]\n",
        "\n",
        "dev_data = data[720:900]\n",
        "\n",
        "test_data = data[900:]"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g9DcRvTjVSdl"
      },
      "source": [
        "class NBC():\n",
        "  def __init__(self):\n",
        "    self.model_dict = {}\n",
        "    self.pos_docs = 0\n",
        "    self.neg_docs = 0\n",
        "    self.tot_docs = 0\n",
        "\n",
        "  def train(self, train_data):\n",
        "    for data in train_data:\n",
        "      label = data[len(data)-2]\n",
        "      positive = True\n",
        "      data = data[:len(data)-3]\n",
        "      words = data.split()\n",
        "      for i in range(len(words)):\n",
        "        words[i] = words[i].replace(\".\",\"\").replace(\",\",\"\").lower()\n",
        "      words = set(words)\n",
        "\n",
        "\n",
        "      if label == '1':\n",
        "        self.pos_docs += 1\n",
        "      elif label == '0':\n",
        "        self.neg_docs += 1\n",
        "        positive = False\n",
        "      self.tot_docs += 1\n",
        "\n",
        "      for word in words:\n",
        "        if word not in self.model_dict:\n",
        "          if positive:\n",
        "            self.model_dict[word] = [1, 1, 0]\n",
        "          else:\n",
        "            self.model_dict[word] = [1, 0, 1]\n",
        "        else:\n",
        "          self.model_dict[word][0] += 1\n",
        "          if positive:\n",
        "            self.model_dict[word][1] += 1\n",
        "          else:\n",
        "            self.model_dict[word][2] += 1\n",
        "\n",
        "  def word_probability(self, word):\n",
        "    return model.model_dict[word][0]/model.tot_docs\n",
        "\n",
        "  def word_given_positive(self, word, smoothing=False):\n",
        "    if smoothing:\n",
        "      return (model.model_dict[word][1] + 1)/(model.pos_docs + 2)\n",
        "    else:\n",
        "      return model.model_dict[word][1]/model.pos_docs\n",
        "\n",
        "  def word_given_negative(self, word, smoothing=False):\n",
        "    if smoothing:\n",
        "      return (model.model_dict[word][2] + 1)/(model.neg_docs + 2)\n",
        "    else:\n",
        "      return model.model_dict[word][2]/model.neg_docs\n",
        "\n",
        "  def positive(self, word, smoothing=False):\n",
        "    return ((self.word_given_positive(word, smoothing=smoothing))*(self.word_probability(word)))/(model.pos_docs/model.tot_docs)\n",
        "\n",
        "  def negative(self, word, smoothing=False):\n",
        "    return ((self.word_given_negative(word, smoothing=smoothing))*(self.word_probability(word)))/(model.neg_docs/model.tot_docs)\n",
        "\n",
        "  def forward(self, test_data, smoothing=False):\n",
        "    words = test_data.split()\n",
        "    for i in range(len(words)):\n",
        "        words[i] = words[i].replace(\".\",\"\").replace(\",\",\"\").lower()\n",
        "    words = set(words)\n",
        "    pos = model.pos_docs/model.tot_docs\n",
        "    neg = model.neg_docs/model.tot_docs\n",
        "    for word in words:\n",
        "      if word in self.model_dict:\n",
        "        pos = pos*self.word_given_positive(word, smoothing)\n",
        "        neg = neg*self.word_given_negative(word, smoothing)\n",
        "      else:\n",
        "        if smoothing:\n",
        "          pos = pos*(1/(model.pos_docs + 2))\n",
        "          neg = neg*(1/(model.neg_docs + 2))\n",
        "        else:\n",
        "          pos = 0\n",
        "          neg = 0\n",
        "\n",
        "    return pos, neg\n",
        "\n",
        "#Return True for positive sentiment, False for negative sentiment\n",
        "  def predict(self, test_data, smoothing=False):\n",
        "    pos, neg = self.forward(test_data, smoothing=smoothing)\n",
        "    if pos > neg:\n",
        "      return True\n",
        "    else:\n",
        "      return False\n",
        "\n",
        "  def test(self, test_data, labels, smoothing=False):\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for i in range(len(test_data)):\n",
        "      if labels[i] == '1':\n",
        "        label = True\n",
        "      else:\n",
        "        label = False\n",
        "      prediction = self.predict(test_data[i], smoothing=smoothing)\n",
        "      if label == prediction:\n",
        "        correct += 1\n",
        "      total += 1\n",
        "\n",
        "    return correct/total\n",
        "          "
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fh427mAXltQX"
      },
      "source": [
        "model = NBC()\n",
        "model.train(train_data)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZrR31iaVu59x",
        "outputId": "dbde458f-9e52-45d5-cb1a-05c53113c6e4"
      },
      "source": [
        "#P('the')\n",
        "prob = model.word_probability('the')\n",
        "print(prob)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.49166666666666664\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sGYttoWCvRre",
        "outputId": "14bbaa60-6d37-48d5-9d9e-bd03ba230f78"
      },
      "source": [
        "#P('the'|Positive)\n",
        "prob = model.word_given_positive('the')\n",
        "print(prob)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.47112462006079026\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hdQO1BzkIEvW",
        "outputId": "ad317a4b-434f-49a5-e43d-ff748f1b97fa"
      },
      "source": [
        "#P('the'|Negative)\n",
        "prob = model.word_given_negative('the')\n",
        "print(prob)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.5173333333333333\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "JFPt_uwy__4S",
        "outputId": "9b461e85-bedb-4faf-fcaf-94f472225dba"
      },
      "source": [
        "test_data[0]"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"Otherwise, don't even waste your time on this.  \\t0\\n\""
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bwRfjWZsM4If",
        "outputId": "121e2eae-0954-45b4-b2ef-5476f8b09ff1"
      },
      "source": [
        "model.predict(test_data[0], smoothing=False)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Si2tHADzPOH3"
      },
      "source": [
        "test_labels = []\n",
        "for x in test_data:\n",
        "  test_labels.append(x[len(x)-2])"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4yrmQqIlPIev",
        "outputId": "c83f734e-67b9-41bc-ef3b-8bfed5cbccfd"
      },
      "source": [
        "model.test(test_data, test_labels, smoothing=False)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.39"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wjsHV_IkReGA"
      },
      "source": [
        "<p>Perform 5-fold cross validation</p>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "knMcqau3TllC"
      },
      "source": [
        "def validate(train_data, dev_data, smoothing=False):\n",
        "  model = NBC()\n",
        "  model.train(train_data)\n",
        "  labels = []\n",
        "  for x in dev_data:\n",
        "    labels.append(x[len(x)-2])\n",
        "    \n",
        "  return model.test(dev_data, labels, smoothing=smoothing)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OGXt3HDfR2bv"
      },
      "source": [
        "def FiveFoldCrossValidation(data, smoothing=False):\n",
        "  accuracy = []\n",
        "  train_data = data[180:900]\n",
        "  dev_data = data[:180]\n",
        "\n",
        "  accuracy.append(validate(train_data, dev_data, smoothing=smoothing))\n",
        "\n",
        "  train_data = data[:180] + data[360:900]\n",
        "  dev_data = data[180:360]\n",
        "\n",
        "  accuracy.append(validate(train_data, dev_data, smoothing=smoothing))\n",
        "\n",
        "  train_data = data[:360] + data[540:900]\n",
        "  dev_data = data[360:540]\n",
        "\n",
        "  accuracy.append(validate(train_data, dev_data, smoothing=smoothing))\n",
        "\n",
        "  train_data = data[:540] + data[720:900]\n",
        "  dev_data = data[540:720]\n",
        "\n",
        "  accuracy.append(validate(train_data, dev_data, smoothing=smoothing))\n",
        "\n",
        "  train_data = data[:720]\n",
        "  dev_data = data[720:900]\n",
        "\n",
        "  accuracy.append(validate(train_data, dev_data, smoothing=smoothing))\n",
        "\n",
        "  return accuracy"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6YCgyjnBUJXb",
        "outputId": "65f03e8e-f69e-4898-b68e-171d6736f59c"
      },
      "source": [
        "accuracy = FiveFoldCrossValidation(data)\n",
        "print(accuracy)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.6666666666666666, 0.65, 0.6277777777777778, 0.5777777777777777, 0.45555555555555555]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MAyKNK5jVYPz"
      },
      "source": [
        "<p>Now we evaluate the effects of smoothing with 5-fold cross validation</p>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_PinTHshVhkC",
        "outputId": "1cb8b99b-13d6-460e-ce15-f3ebf12e1bcf"
      },
      "source": [
        "smoothing_accuracy = FiveFoldCrossValidation(data, smoothing=True)\n",
        "print(smoothing_accuracy)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.8666666666666667, 0.8611111111111112, 0.8666666666666667, 0.9, 0.7277777777777777]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8xcLXVMMWjwQ"
      },
      "source": [
        "<p>From the experiments with cross validation we can determine that the best model is using the fourth train/dev split. Using this model we can derive the top ten words that predict positive and likewise the top ten words that predict negative. </p>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KOLgk193W-OX"
      },
      "source": [
        "train_data = data[:540] + data[720:900]\n",
        "dev_data = data[540:720]\n",
        "\n",
        "model = NBC()\n",
        "model.train(train_data)\n",
        "\n",
        "pos = []\n",
        "neg = []\n",
        "for word in model.model_dict:\n",
        "  pos.append((model.positive(word, smoothing=True), word))\n",
        "  neg.append((model.negative(word, smoothing=True), word))\n"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q-zSSM39YJHY"
      },
      "source": [
        "def sort_help(x):\n",
        "  return x[0]\n",
        "\n",
        "pos.sort(reverse=True, key=sort_help)\n",
        "neg.sort(reverse=True, key=sort_help)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SWNcKFLLaXcP"
      },
      "source": [
        "<p>Top ten words that predict positive and their probabilities P(Positive|word) </p>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Hwqefl0Z1Eq",
        "outputId": "09bcdaae-7946-4376-f28d-51005880324d"
      },
      "source": [
        "print(pos[:10])"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(0.5334168650545044, 'the'), (0.2501942112517228, 'a'), (0.24207492795389043, 'and'), (0.1912709351376185, 'of'), (0.18322683038884016, 'is'), (0.15394896211836445, 'this'), (0.115774965543165, 'i'), (0.10162469197677818, 'it'), (0.09197677818151442, 'to'), (0.08896128304723717, 'in')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YrideKJIakPI"
      },
      "source": [
        "<p>Top ten words that predict negative and their probabilities P(Negative|word) </p>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wXt9kEkHapec",
        "outputId": "e897d310-0e23-4e99-b076-f5c1ac522b8d"
      },
      "source": [
        "print(neg[:10])"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(0.506206896551724, 'the'), (0.17875862068965517, 'a'), (0.1724491600353669, 'and'), (0.16499381078691425, 'of'), (0.14500442086648982, 'is'), (0.12767462422634834, 'this'), (0.09687002652519892, 'i'), (0.09276038903625108, 'it'), (0.06930503978779841, 'in'), (0.06878160919540229, 'to')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yqYSFZc9bOG_"
      },
      "source": [
        "<p>It can be seen from the results that the same set of words are the top predictors for both positive and negative sentiment. This is because these are very common words in the English language. These words are included in a set of words called \"Stop Words\" and these words are typically filtered out of the dataset. </p>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "94mjD_FRbrDx"
      },
      "source": [
        "<p>Finally we test our best model on the last 10% of the dataset that was withheld for testing. </p>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BDZby6Njb6VF",
        "outputId": "53164257-f851-42f5-8b44-bf37cf8788ff"
      },
      "source": [
        "model.test(test_data, test_labels, smoothing=True)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.71"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    }
  ]
}